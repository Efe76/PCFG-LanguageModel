{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5756f8-4e1d-4914-8eda-91126175aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import numpy \n",
    "import random\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "from nltk import CFG, Nonterminal, Production\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.parse.corenlp import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3b71ed-12d9-41a3-a41b-784377678226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API connecten Terminal\n",
    "#java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a74356e-a016-40bc-9bdd-cd4dfe11799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff1b0cb-d898-44ec-85da-9b821afaf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voeg txt file toe\n",
    "with open('HobbitKI.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadc6ba3-5f9a-4107-9ee8-28426d94c548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "illegal_words = [\"\\ufeffin\", \"/xad\", \";\"]\n",
    "\n",
    "def preprocesser(given_text):\n",
    "    #Doel is om gegeven text op te schonen van puncties en illegale woorden\n",
    "    #Returnt text in een list met sentences\n",
    "    \n",
    "    # Lowercasing\n",
    "    given_text = given_text.lower()\n",
    "    sentences = sent_tokenize(given_text)\n",
    "    \n",
    "    # Alle puncties\n",
    "    punctuations = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # Preprocessing\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Puncties verwijderen\n",
    "        sentence = sentence.translate(punctuations)\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        #Check of zin illegaal woord bevat, zo ja voeg zin niet toe.\n",
    "        #Nadeel, 1 illegale woord zorgt dat een heel zin verwijderd wordt\n",
    "        illegal_word = False\n",
    "        for word in words:\n",
    "            if word in illegal_words:\n",
    "                illegal_word = True\n",
    "                break\n",
    "\n",
    "        #Als zin geen illegaal woord bevat voeg toe\n",
    "        if not illegal_word:\n",
    "            preprocessed_sentences.append(' '.join(words))\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "    \n",
    "# # Output\n",
    "# preprocessed = preprocesser(text)\n",
    "\n",
    "# for sentence in preprocessed:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafc7088-01de-4084-8b13-83fdd57ed525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 CFG maken met stanford API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75ee5ec-c7af-4439-ac04-62cc83ba406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (PRN\n",
      "      (S\n",
      "        (NP (PRP he))\n",
      "        (VP\n",
      "          (VBD lit)\n",
      "          (PRT (RP up))\n",
      "          (NP (PRP$ his) (NN wand))\n",
      "          (SBAR\n",
      "            (IN as)\n",
      "            (S\n",
      "              (NP (PRP he))\n",
      "              (VP\n",
      "                (VBD did)\n",
      "                (NP\n",
      "                  (NP\n",
      "                    (NP (DT that) (NN day))\n",
      "                    (PP\n",
      "                      (IN in)\n",
      "                      (NP (NNS bilbos) (NML (NN dining) (NN room)))))\n",
      "                  (SBAR\n",
      "                    (WHNP (WDT that))\n",
      "                    (S\n",
      "                      (VP\n",
      "                        (VBD seemed)\n",
      "                        (ADVP (RB so) (RB long))\n",
      "                        (PP\n",
      "                          (PP (IN ago) (PP (IN if) (NP (PRP you))))\n",
      "                          (CC and)\n",
      "                          (PP (IN by) (NP (PRP$ its) (NN light))))))))))))))\n",
      "    (NP (PRP they))\n",
      "    (VP\n",
      "      (VBD explored)\n",
      "      (NP (DT the) (NN cave))\n",
      "      (PP (IN from) (NP (NN end)))\n",
      "      (PP (IN to) (NP (NN end))))))\n"
     ]
    }
   ],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')\n",
    "#Zinnen:\n",
    "#1. the architect obtained a sketchbook or a pencil from the landlord (1 normale zin, voor als basis)\n",
    "#2. he lit up his wand as he did that day in bilbos dining room that seemed so long ago if you and by its light they explored the cave from end to end\n",
    "#3. it had a perfectly round door like a porthole painted green with a shiny yellow brass knob in the exact middle\n",
    "#4. i suppose hobbits need some description nowadays since they have become rare and shy of the big people as they call us\n",
    "\n",
    "line = \"he lit up his wand as he did that day in bilbos dining room that seemed so long ago if you and by its light they explored the cave from end to end\"\n",
    "\n",
    "# Parse tree\n",
    "parse_tree = next(parser.raw_parse(line))\n",
    "\n",
    "# Output\n",
    "print(parse_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa97530-473c-4506-835d-709010c8027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 55 productions (start state = S)\n",
      "    S -> VP\n",
      "    VBD -> 'seemed'\n",
      "    IN -> 'from'\n",
      "    S -> NP VP\n",
      "    NN -> 'wand'\n",
      "    WHNP -> WDT\n",
      "    VBD -> 'explored'\n",
      "    PP -> PP CC PP\n",
      "    PRP -> 'you'\n",
      "    PRP -> 'they'\n",
      "    VBD -> 'lit'\n",
      "    SBAR -> WHNP S\n",
      "    CC -> 'and'\n",
      "    IN -> 'to'\n",
      "    PRN -> S\n",
      "    RB -> 'so'\n",
      "    IN -> 'if'\n",
      "    DT -> 'the'\n",
      "    RB -> 'long'\n",
      "    WDT -> 'that'\n",
      "    S -> PRN NP VP\n",
      "    NP -> DT NN\n",
      "    NML -> NN NN\n",
      "    NN -> 'day'\n",
      "    SBAR -> IN S\n",
      "    IN -> 'as'\n",
      "    NP -> NP PP\n",
      "    IN -> 'by'\n",
      "    NN -> 'dining'\n",
      "    RP -> 'up'\n",
      "    NN -> 'cave'\n",
      "    VP -> VBD PRT NP SBAR\n",
      "    IN -> 'in'\n",
      "    NP -> NP SBAR\n",
      "    NN -> 'room'\n",
      "    VP -> VBD NP PP PP\n",
      "    DT -> 'that'\n",
      "    NNS -> 'bilbos'\n",
      "    NP -> NN\n",
      "    VP -> VBD ADVP PP\n",
      "    PP -> IN PP\n",
      "    NP -> PRP\n",
      "    VP -> VBD NP\n",
      "    IN -> 'ago'\n",
      "    PRT -> RP\n",
      "    PRP -> 'he'\n",
      "    NN -> 'end'\n",
      "    VBD -> 'did'\n",
      "    NP -> NNS NML\n",
      "    NN -> 'light'\n",
      "    PP -> IN NP\n",
      "    NP -> PRP$ NN\n",
      "    ADVP -> RB RB\n",
      "    PRP$ -> 'his'\n",
      "    PRP$ -> 'its'\n"
     ]
    }
   ],
   "source": [
    "def tree_to_cfg(tree, start_symbol='S'):\n",
    "    cfg_rules = set()\n",
    "\n",
    "    def extract_rules(t):\n",
    "        if isinstance(t, Tree):\n",
    "            if t.label() != \"ROOT\":\n",
    "\n",
    "                rule = t.productions()[0]\n",
    "                cfg_rules.add(rule)\n",
    "\n",
    "            for child in t:\n",
    "                extract_rules(child)\n",
    "\n",
    "    extract_rules(tree)\n",
    "    \n",
    "    grammar = CFG(start=Nonterminal(start_symbol), productions=list(cfg_rules))\n",
    "    return grammar\n",
    "\n",
    "# Convert parse tree to CFG\n",
    "cfg = tree_to_cfg(parse_tree)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a10410-73ee-44bd-9818-1d338664c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordandtag_frequencies(sentences):\n",
    "    #Doel is om het aantal word en bijbehorende tag frequencies te tellen\n",
    "    #Om hier later bij generate_pcfg een probability van te maken\n",
    "    \n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tagged = nltk.pos_tag(word_tokenize(sentence))\n",
    "        \n",
    "        for word, tag in tagged:\n",
    "            tag = tag.replace('$', 'DOLLAR')\n",
    "            \n",
    "            # Tel aantal woorden dat bij het tag behoort\n",
    "            # Als tag nog nieet in dict, voeg toe.\n",
    "            if tag not in word_frequencies:\n",
    "                word_frequencies[tag] = {}\n",
    "            \n",
    "            # Value is een dictionary bestaande uit een woord en zijn \"count/occurences)\n",
    "            if word not in word_frequencies[tag]:\n",
    "                word_frequencies[tag][word] = 0\n",
    "            word_frequencies[tag][word] += 1\n",
    "            \n",
    "    return word_frequencies\n",
    "    \n",
    "# # Output\n",
    "# word_frequencies = wordandtag_frequencies(preprocessed)\n",
    "# print(word_frequencies)\n",
    "\n",
    "def generate_pcfg(word_frequencies):\n",
    "    # PCFG creeren, door de probability aand rules te voegen\n",
    "\n",
    "    #Verkregen CFG uit stanford API\n",
    "    grammar_dict = {\n",
    "        'S': [\"NP VP\"],\n",
    "        'NP': [\"DT NN\", \"NP CC NP\", \"PRP\", \"NP PP\", \"DT JJ JJ NN NN\", \"DT JJ NN\", \"DT ADJP NN\", \"DT NN\", \"NNS\", \"PRP\", \"ADJP\", \"NN\", \"NP SBAR\", \"PRP NN\"],\n",
    "        'PP': [\"IN NP\", \"IN PP\", \"IN NP\"],\n",
    "        'VP': [\"VBD NP PP\", \"VBD NP SBAR\", \"VBD ADJP PP\", \"VBP NP\", \"VBP NP ADVP SBAR\", \"VBP SBAR\", \"VBN NP SBAR\", \"VBD NP\", \"VBD PRT NP SBAR\", \"VBD NP PP PP\", \"VBD NP PP\"],\n",
    "        'ADJP': [\"RB JJ\", \"JJ\", \"JJ CC JJ\"],\n",
    "        'SBAR': [\"IN S\", \"S\", \"WHNP S\", \"IN S\"],\n",
    "        'ADVP': [\"RB\", \"RB RB\"],\n",
    "        'PRT': [\"RP\"],\n",
    "        'NML': [\"NN NN\"],\n",
    "        'PRN': [\"S\"],\n",
    "        'WHNP': [\"WDT\"]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Probabilities berekenen van zowel grammar als lexical rules\n",
    "    prob_grammar_rules = []\n",
    "\n",
    "    #Grammar rules probabilties\n",
    "    for key, productions in grammar_dict.items():\n",
    "        total = len(productions)\n",
    "        for production in set(productions):\n",
    "            count = productions.count(production)\n",
    "            prob = count / total\n",
    "            \n",
    "            #Programma werkt niet met wetenschappelijke notatie,\n",
    "            #Dus afgerond op 6 decimalen, kan nog aangepast worden.\n",
    "            prob_grammar_rules.append(f\"{key} -> {production} [{prob:.6f}]\")\n",
    "\n",
    "    #Lexical rules probabilities\n",
    "    for tag, words in word_frequencies.items():\n",
    "        total = sum(words.values())\n",
    "        for word, count in words.items():\n",
    "            probability = count / total\n",
    "            prob_grammar_rules.append(f\"{tag} -> '{word}' [{probability:.6f}]\")\n",
    "    \n",
    "\n",
    "    return prob_grammar_rules\n",
    "# # Output\n",
    "# pcfg_rules = generate_pcfg(word_frequencies)\n",
    "# print(pcfg_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "613dfc2a-8b1b-49d3-8272-928684e842e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the big lair swirled it and it at existence into the beautiful girion and that time\n",
      "it glimpse are a mother nor low and easy here not which he had the safe ridge with the unfortunately flat distance\n",
      "a second last misty credit said the happy morning he came suddenly annoyed at in it rent and the slow gold\n",
      "the mad gem errand return much of unpleasant and heap sat recommendation on a warrant\n",
      "time was another glad or high head ingenious chiefly any swung then far the lake­town business said he in us\n",
      "the i and small floor i you gollum was he bilbo like on yet large\n",
      "a creepsy have of they think with the rug left disorder dawn know them perhaps nearer the last gandalf am the treasure\n",
      "a old bat­cloud are the own dont spoke they that of height before though on unhappy\n",
      "he and the now huge village until i said the together southern wealth for against the night\n",
      "goblins saw we from ears mean that the course black­haired another mountain until the i are which bit suppose enough nassty\n"
     ]
    }
   ],
   "source": [
    "def top_down_expansion(pcfg, sentence, non_terminal):\n",
    "    #Expands de grammar rules van links naar rechts, tot het een terminal bereikt,\n",
    "    #die voegen we toe aan onze sentence, tot alle non_terminals een terminal bereiken\n",
    "\n",
    "    # Zoek voor alle regels, die de non_terminal bevat \n",
    "    # VB. S -> NP VP | NP VBZ VP | etc.\n",
    "    if non_terminal in pcfg._lhs_index:\n",
    "        possible_rules = pcfg._lhs_index[non_terminal]\n",
    "\n",
    "        # Neem de probabilities van elk production \n",
    "        # VB. S -> NP VP [1.0}\n",
    "        probabilities = []\n",
    "        for left_most_rule in possible_rules:\n",
    "            probabilities.append(left_most_rule.prob())\n",
    "\n",
    "        # Kies een RHS rule gebaseerd op kansen\n",
    "        chosen_path = random.choices(possible_rules, weights=probabilities)[0]\n",
    "        # Expand de meest linker non-terminal \n",
    "        # btw .rhs() werkt niet op list\n",
    "        for left_most_rule in chosen_path.rhs():\n",
    "            top_down_expansion(pcfg, sentence, left_most_rule)\n",
    "    else:\n",
    "        # Als geen mogelijke rules, dan is het een terminal/woord\n",
    "        # Voeg toe aan sentence\n",
    "        terminal = non_terminal\n",
    "        sentence.append(str(terminal))\n",
    "\n",
    "def generate_sentence(pcfg):\n",
    "    # Genereerd de sentence, door top_down_expansion the runnen en de woorden toe te\n",
    "    # voegen aan sentence, verder join het tot een string\n",
    "    sentence = []\n",
    "    top_down_expansion(pcfg, sentence, pcfg.start())\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def main(text):\n",
    "    #Verkrijg processed sentences\n",
    "    preprocessed_sentences = preprocesser(text)\n",
    "    \n",
    "    #Verkrijg de PCFG, met bijbehorende probabilities\n",
    "    #Returns probabilties\n",
    "    word_frequencies = wordandtag_frequencies(preprocessed_sentences)\n",
    "    #Returns pcfg in list type\n",
    "    pcfg_rules_list = generate_pcfg(word_frequencies)\n",
    "    #Returns pcfg als str\n",
    "    pcfg_rules_string = '\\n'.join(pcfg_rules_list)  \n",
    "    #Returns de PCFG als grammar\n",
    "    pcfg = nltk.PCFG.fromstring(pcfg_rules_string)\n",
    "\n",
    "    # Genereet de sentences\n",
    "    sentences = []\n",
    "    # Aanpasbaar, ligt eraan hoeveel sentences je wilt\n",
    "    while len(sentences) < 10:\n",
    "        sentence = generate_sentence(pcfg)\n",
    "\n",
    "        # Dit is geen must, maar anders is er groot kans dat het programma\n",
    "        # lange zinnen creert met veel recursion door gebruik van \"and\" en \"but\"\n",
    "        # Dit valt ook aan te passen, ligt eraan hoe lang je de sentences wilt\n",
    "        word_count = len(sentence.split())\n",
    "        if 15 <= word_count <= 25:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Output\n",
    "generated_sentences = main(text)\n",
    "for sentence in generated_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fcecec-d052-47ec-a855-d63e4dcecf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b49f1-5df3-40f6-af28-4e0f3d410ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
